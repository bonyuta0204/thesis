# 第1章 序論

fine-tuningは畳み込みニューラルネットワークの学習に用いられる手法であり, あるタスクのために学習された畳み込みニューラルネットワークの重みを初期値として用いることにより, 別のタスクの学習を行う手法である. 通常の畳み込みニューラルネットワークの学習には大量のデータが必要となるが, fine-tuningを用いる場合, 比較的小規模のデータでも学習を行えるため, 特に計算機による画像識別タスクにおいて一般的に用いられている技術である. 

fine-tuningを行う際には, 元となるタスクの学習を通して, 畳み込みニューラルネットワークがfine-tuningの対象となるタスクにおいて有用となる特徴を抽出している必要がある. 一般に, 画像識別の領域においては画像識別タスクを学習した学習モデルがこのような特徴量を抽出しているとされ, 最も一般的にfine-tuningの際の元のモデルとして使用されており, セグメンテーションやキャプションの生成など多くの画像認識タスクにおいて成果を上げている. 

また, fine-tuningの有用性は二次元画像の識別だけではなく, 動画の認識においても効果的であることが示されている. 一例として動画中の動詞判別タスクにおいて, 大量のデータを有するkineticsデータセットで学習したモデルを元として, より小規模なデータセットを学習することが可能であることが示されている. 一方で, 三次元の畳み込みニューラルネットワークにおいては, 動詞判別タスクを学習した畳み込みニューラルネットワークのfine-tuningによって, 別のタスクを学習できるかは十分に知られていない. また, 動画よりも多くのデータセットや先行研究が存在する画像認識において学習された畳み込みニューラルネットワークとの比較が行われておらず, 動画を対象とした動詞判別タスク以外のタスクをfine-tuningを用いて行う際に元とする学習済み畳み込みニューラルネットワークの選択方法は十分に研究されていないのが現状である. 

そこで, 本研究では動画中の物体判別タスクを対象として, 3次元畳み込みニューラルネットワークのfine-tuningを行う際に利用する学習済みの畳み込みニューラルネットワークの比較を行った. 特に, 動画中の物体識別タスクにおいて, 二次元学習の動画識別タスクで学習されたモデルと, 3次元の動画中の動詞判別タスクで学習された畳み込みニューラルネットワークを比較することにより, タスクの特性から抽出された特徴と, 画像に時間方向が加わった動画というデータの特性から抽出された特徴を比較することにより3次元畳み込みニューラルネットワークを用いた畳み込みニューラルネットワークにおけるfine-tuningの特性について検証を行う. 第2章の第1節では, 今回の検証に使用した畳み込みニューラルネットワークについての詳細を解説する. 第2章の2,3節では, 今回の検証に用いたデータセットと学習の際の手続きについて述べる. 第2章の4節では, 比較検討の方法論について述べた後, 第3, 4章では, 比較の結果および検証を行い, 三次元畳み込みニューラルネットワークのfine-tuningについて考察を行う. 

# 第2章 方法

## 2.1 畳み込みニューラルネットワーク
本検証では, 訓練済み畳み込みニューラルネットワークをfine-tuningし, 物体判別タスクの検証を行った. 

### 2.1.1 二次元畳み込みニューラルネットワーク
二次元の畳み込みニューラルネットワークで画像中の物体判別タスクを行うネットワークとして,ImageNet (引用) を用いた1000クラス判別タスクでpre-trainされた ResNets (引用) を用いた. 本検証においては, 50層のResNetsを用いた.

### 2.1.2 三次元畳み込みニューラルネットワーク
本研究では三次元の畳み込みニューラルネットワークとして, 画像中の物体判別タスクでpre-trainされた二次元の畳み込みニューラルネットワークを拡張した時空間畳み込みニューラルネットワークと, 同様のネットワークを動画中の動詞判別でpre-trainしたネットワークを用いた. 

#### 2.1.2.1 畳み込みニューラルネットワークの拡張
時空間方向の畳み込みを行う三次元の畳み込みニューラルネットワークで, 2次元画像中の物体判別タスクを行うものとして, I3D (引用) ネットワークを用いた. 

I3Dネットワークは, 訓練済みの二次元畳み込みニューラルネットワークを3次元に拡張することにより作られる時空間畳み込みニューラルネットワークである. 拡張は, 時空間畳み込みニューラルネットワークの作成と, 2次元畳み込みニューラルネットワークからの学習済みの重みの転移によって行われる. 時空間畳み込みニューラルネットワークは, 畳み込みニューラルネットワークの畳み込み層とプーリング層に時間方向の次元を加えることにより作成される. ネットワークを作成した後の重みの転移は, 時空間畳み込みニューラルネットワークに二次元の同じ画像を繰り替し, 作成された動きがない動画 (boring-video) を入力した時の出力が, もとの二次元畳み込みニューラルネットワークに同じ画像を入力した時の出力と等しくなるような制限をみたすように行う. 

本検証では二つの方法で, 上記の制約を満たす拡張を行った. それぞれの方法において, 時空間畳み込みニューラルネットワークの畳み込み層の重みは, 変換前の2次元畳み込みニューラルネットワークにおいて対応する畳み込み層の重みから転移を行った. 一つ目は, 変換する層の時間軸方向の大きさがNのとき, 対応する畳み込み層の重みを時間方向にN回繰り返した後に, 1/N倍することにより時空間ネットワークの重みの初期化を行った. 二つ目の方法では, 時空間畳み込みニューラルネットワークの重みをすべて0で初期化を行った後に, 時間軸において中央に位置するフィルターのみを対応にする二次元畳み込みニューラルネットワークの重みを用いて初期化することによって転移を行った. 本研究においては, 前者を平均化拡張, 後者を中心化拡張と呼ぶ. 

本検証においては, 画像中の物体判別にpre-trainされた時空間畳み込みニューラルネットワークとして, ImageNetでpre-trainされたResNets50 をそれぞれ, 平均化拡張, 中心化拡張によって時空間畳み込みニューラルネットワークに拡張したものを用いた. 

#### 2.1.2.2 動詞判別時空間畳み込みニューラルネットワーク

## 2.2 データセット
### 2.2.1 Moments In Time データセット
I3Dの訓練, および検証にはMomets In Timeデータセット (引用) から抽出した1250件の動画データ及び, 動画に対応する物体カテゴリラベルを使用した. Moments In Timeデータセットは100万枚以上の3秒間の動画に339種類のアクションのカテゴリが動詞名で一つずつ付けられたデータセットであり, 同様のものとしては最大規模のデータセットである.

### 2.2.2 データセットの抽出
本研究では, 動詞ラベルではなく動画中の物体カテゴリラベルを利用するため, Moments In Timeデータセットから訓練, バリデーション用のデータとして1200件, テスト用に50件のデータを抽出した. 抽出に際して訓練, バリデーション用データは150のアクションカテゴリから8件ずつ, テスト用データはその内50のカテゴリから1件ずつのデータを抽出した. 
抽出した動画に対する物体カテゴリのラベリングは, それぞれの動画中に確認できる物体のラベルを複数つける形で行った.
ラベリングを行った結果, 193の物体カテゴリがラベルとして与えられ, 1動画あたりの平均ラベル数は1.41であった. 本研究では, この内出現頻度上位20ラベルのみを用いた. 上位20のラベルがつけられた動画は937件, 1動画あたりの平均のラベル数は1.25であった.
ラベルが付けられた動画データは, 全て時間が3秒間, フレーム数90枚, 解像度は縦256画素, 横256画素のサイズの物であった. 本検証においては, 90フレームから均等に32フレームを抽出して用いた.
## 2.3 物体判別学習

### 2.3.1二次元畳み込みニューラルネットワーク
二次元の畳み込みニューラルネットワークは以下の方法で訓練を行った. ニューラルネットワークへの入力は, 作成したデータセット中の動画データの32フレームをそれぞれ一枚の画像として入力を行った. 入力は全動画の全フレームをランダムにシャッフルした後, 16枚を1バッチとして入力を行った. また, それぞれの入力画像に対して, 左右, 上下の反転をおこなった後, 256 x 256の解像度の画像から224 x 224の解像度の画像をランダムな位置で切り抜く前処理を行った. 

また, 学習時の条件は以下のものを用いた. 損失関数には最終層の出力にシグモイド関数を適用した各ラベルの予測値と, 真のラベルとのクロスエントロピーの全ラベル間での平均を用いた. 最適化手法としては, Momentum SGD (引用?) を, Momentumの値を0.9として使用した. 重みは4バッチ毎に勾配を蓄積し, その勾配を用いて更新した. 本研究においては, 一回の重みの更新を1ステップと呼ぶ. 学習率は初期値として0.01を用い, それぞれ300ステップ, 1000ステップの学習後に0.1倍した. また, 学習の際はWeight Decay (引用？) を用いた正則化を行った. 

### 2.3.2 時空間畳み込みニューラルネットワーク
二次元の畳み込みニューラルネットワークは以下の方法で訓練を行った. ニューラルネットワークへの入力は, 1動画を1バッチとして入力を行った. また, それぞれの入力動画に対して, 左右, 上下の反転をおこなった後, 256 x 256の解像度の動画から224 x 224の解像度の動画をランダムな位置で切り抜く前処理を行った. 

また, 学習時の条件は, 上述の二次元畳み込みニューラルネットワークと同様のものを用いた. 


## 2.3 検証
物体判別タスクの成績は以下の方法で検証した. 

### 2.3.1 評価方法
畳み込みニューラルネットワークの比較は, データセットのテストデータを用いて行った. 畳み込みニューラルネットワークへの入力は256 x 256 の解像度の画像及び動画から中央の224 x 224を切り抜いたものを使用した. 畳み込みニューラルネットワークの最終層の値を, それぞれの対応するラベルの予測値として評価を行った. 

### 2.3.2 評価指標
ニューラルネットワークによる予測の評価は, 20それぞれのラベルの物体が予測画像に含まれているかの二値判別としてAUC (Area Under Curve) を用いて行った. 20ラベルそれぞれについて, テストセット全体の予測値と真のラベルを用いてラベル毎のAUCを算出した. 

# 3章 結果
Moments In Timeデータセットを用いて, マルチラベル判別問題における二次元畳み込みニューラルネットワークと時空間畳み込みニューラルネットワークの動画中の物体識別タスクにおける成績の評価を行った. 

### 学習曲線
畳み込みニューラルネットワークの学習時の損失の比較を行った. 

図xは, ニューラルネットワークの学習中の損失の比較である. 損失は最終層の各カテゴリ毎の予測における交差エントロピー誤差を各カテゴリにおいて平均することで求めた. 全ての畳み込みニューラルネットワークにおいて学習初期に急激に損失が減少した後に学習が収束した. 二次元畳み込みニューラルネットワークにおいてのみ, 訓練データでの損失とテストデータでの損失に大きな差が見られ, それ以外の畳み込みニューラルネットワークにおいては, 訓練データとテストデータにおいて損失の値は大きな差は見られなかった. 



### 判別結果
マルチラベル判別問題の結果の比較を行った. 

図xは, いくつかのカテゴリに対する予測のROC曲線を比較したものである. 中心化拡張によって拡張された時空間畳み込みニューラルネットワーク, 動詞判別時空間畳み込みニューラルネットワーク, 二次元畳み込みニューラルネットワークにおいては, ROC曲線はチャンスレベルのものと同等の結果を示した. 一方で, 平均化拡張によって拡張された時空間畳み込みニューラルネットワークは, 他の畳み込みニューラルネットワークよりも判別成績がよいことが分かる. 



また, 図xは畳み込みニューラルネットワークの予測値から算出した, ラベルごとのAUCである. 

画像識別ニューラルネットワークにより判別結果では, 各カテゴリのAUCはおおよそ0.5となっており, 学習に失敗していることが分かる. また, 中心化拡張によって拡張された時空間畳み込みニューラルネットワークにおいては, 画像識別ニューラルネットワークよりも値の変動が大きいものの, おおよそAUCは0.5付近の値を取っており, 学習は成功していないことが分かる. 平均化拡張によって拡張された時空間畳み込みニューラルネットワークにおいては, 前述の2つのネットワークよりも高いAUCを示しており, 学習が一定成功していることがわかる. 動詞判別時空間畳み込みニューラルネットワークに関しては他のものよりも総合的にAUCが低い結果となった. 

# 第4章 考察



本研究では, 複数の方法で重みを設定した畳み込みニューラルネットワークのfine-tuningを行い, 動画中の物体判別タスクの成績の比較を行った. 以下では, まず, それぞの畳み込みニューラルネットワークを用いてfine-tuningお行った際の性質について考察を行う. その後に, 4つの畳み込みニューラルネットワークの比較から時空間畳み込みニューラルネットワークにおけるfine-tuningの特性についての考察を行う. 

fine-tuningをベースにした動画中の物体判別タスクにおいては, 今回比較を行った4つの畳み込みニューラルネットワークの中で, 平均化拡張によって二次元画像識別タスクで訓練を行ったもののみが学習に成功したと言える. 図xで示された通り, 3つのネットワークに関しては, ROC曲線や, AUCの値は総合的に見てチャンスレベルに近く, 物体判別タスクの学習は成功しなかった. しかし, 図xの学習曲線から, 学習が成功しなかった原因が二次元の畳み込みニューラルネットワークと3次元畳み込みニューラルネットワークにおいて異なることが明らかになった. 

二次元の畳み込みニューラルネットワークからfine-tuningを行った場合は, 訓練データに対する判別誤差が, テストデータに対する判別成績を大きく下回る過学習が起きることが分かった. これは, 動画中のフレームを画像として切り出して訓練を行う際に, 画像としての類似度が非常に高い画像が複数入力されるという特徴によって引き起こされていると考えられる. 一方, 時空間畳み込みニューラルネットワークからfine-tuningを行った場合は, 訓練データに対する判別誤差とテストデータに対する判別誤差の間の乖離は起きず, 双方とも判別誤差が初期段階で一定となることが明らかになった. 

時空間畳み込みニューラルネットワークの中で平均化拡張で拡張された画像識別畳み込みニューラルネットワークのみが物体判別タスクにおいて成績が高かった原因としては, 平均化拡張においては初期の畳み込みニューラルネットワークの重みからの変化量が小さくても, 新しいタスクの学習が行えるという可能性が考えられる. すでに物体判別のタスクで学習されているネットワークを拡張する場合, 2章で述べた中心化拡張と平均化拡張という2つの手法を用いることができるが, 前者の中心化拡張の場合は畳み込み層の重みの大部分の値が 0 という状態から訓練を行う必要があるため, 本検証のようにデータ量が限られている条件においては十分に重みを更新できなかった可能性がある. また, 動詞判別タスクで訓練された時空間畳み込みニューラルネットワークを用いた場合に関しても, 画像判別とは別のタスクで訓練されていたため, 今回のデータ量では十分に重みが変化しなかった可能性も考えられる. 本検証においては, 学習済みのニューラルネットワークの重みについては定量的な評価が行えていないため, 今後の課題として, 学習済み畳み込みニューラルネットワークの重みの分析を行う必要がある. 

以上のような比較から, 時空間畳み込みニューラルネットワークにおけるfine-tuningにおいては以下のような特性があると考えられる. まず, 動画を扱う畳み込みニューラルネットワークとしては二次元の畳み込みニューラルネットワークと時空間畳み込みニューラルネットワークが挙げられるが, 本検証に用いた, 比較的小規模のデータ量を用いた場合には二次元の畳み込みニューラルネットワークは過学習に陥る傾向がある. 一方, 時空間畳み込みニューラルネットワークにおいては, 訓練データを含め判別成績が向上しにくいという問題がある. 

本研究では, 動画を用いたタスクにおける前述した問題を緩和する方法として, 同様のタスクで訓練された二次元畳み込みニューラルネットワークを平均化拡張によって時空間畳み込みニューラルネットワークに拡張したネットワークを初期値として用いたfine-tuningが有用であることが示唆された. 今後の課題としては, fine-tuningを行った後の畳み込みニューラルネットワークの学習済みの重みの定量的な分析を行い, 平均化拡張のみが好成績を残したメカニズムを検証することが挙げられる. 

# 第5章 結論
本研究では, 動画中の物体判別タスクの学習のための動画のfine-tuningの特性を調査するために, 異なる学習済み畳み込みニューラルネットワークを用いてfine-tuningを行い, 判別結果を比較した. その結果, 動画中の物体判別タスクのfine-tuningにおいては, 同様の画像判別タスクで学習済みの二次元畳み込みニューラルネットワークを平均化拡張を用いて時空間畳み込みニューラルネットワークに拡張したネットワークを元としてfine-tuningを行うことでタスクの学習に成功するという結果が得られた. 

# 謝辞
本研究を⾏うにあたり，脳情報学研究室の神⾕之康教授，間島慶助教には数々のご指導, ご協力を頂きました. 研究のみならず, 多岐に渡ってご支援頂いたことに心より感謝しております. ATR 脳情報研究所の塚本光昭研究技術員には，研究室の計算機環境の構築および研究を円滑に進める上での数々のサポートをしていただき感謝いたします．京都⼤学情報学研究科修⼠課程1 回の白川健さんには, 対象データの準備や解析方法のサポートをしていただきました. 最後に研究に対して⽀援してくださった脳情報学研究室，ATR 脳情報研究所の皆様に感謝いたします．